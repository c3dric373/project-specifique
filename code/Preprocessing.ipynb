{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SetUp directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_directory = '../data/'\n",
    "if(not os.path.exists((data_directory))):\n",
    "     os.makedirs(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SetUp Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "corpus_file = 'corpus_check_long.csv'\n",
    "corpus_path = data_directory + corpus_file\n",
    "# We will create a temporary file with the results of the preprocessing this file will be deleted after \n",
    "#the execution of the script\n",
    "temp_file_eval = \"../data/evalFile.txt\"\n",
    "\n",
    "# File Name where we will store the training data \n",
    "train_path = data_directory + 'trainFile.txt'\n",
    "\n",
    "# File name where we will store the evaluation data\n",
    "eval_file = data_directory + 'eval.csv'\n",
    "\n",
    "# Prediction File \n",
    "prediction_file = data_directory + 'prediction.csv'\n",
    "\n",
    "# Name of the column storing the article \n",
    "article = 'corpus'\n",
    "\n",
    "utilities_path = '../utilities/'# DataSetPath \n",
    "prediction_path = utilities_path + 'groupC_scrap.obj'\n",
    "prediction_csv_path = utilities_path + 'prediction.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets to preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "file = open(prediction_path, 'rb') \n",
    "df_prediction = pd.DataFrame(pickle.load(file))\n",
    "df = pd.read_csv(corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrupt_data(df):\n",
    "    indexNames = []\n",
    "    for counter,data in enumerate(df.iterrows()):\n",
    "        i, row = data\n",
    "        tmp = df.corpus[i]\n",
    "        if ((\"�\") in tmp) or (len(tmp.split())<50):\n",
    "            indexNames.append(i)\n",
    "    return indexNames\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(dataframe):\n",
    "    # Remove corrupt Data and filter articles that have less than 50 words\n",
    "    indexNames = get_corrupt_data(dataframe)  \n",
    "    dataframe.drop(indexNames , inplace=True)\n",
    "    \n",
    "    # Filter companies that have at least 7 articles\n",
    "    top = dataframe[\"siren\"].value_counts()\n",
    "    top = top.where(top>=7).dropna()\n",
    "    topList = list(top.index)\n",
    "    dataframe = dataframe[dataframe[\"siren\"].isin(topList)]\n",
    "    \n",
    "    # Filter articles longer than 1,000,000 characters\n",
    "    dataframe = dataframe[dataframe[article].astype(str).map(len)<1000000]\n",
    "    \n",
    "    return dataframe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize \n",
    "def cleaning(doc):\n",
    "    doc = doc.replace('\\n', ' ')\n",
    "    doc = doc.replace('\\r\\n', ' ')\n",
    "    doc = doc.replace('\\r', ' ')\n",
    "    doc = doc.replace('\\t', ' ')\n",
    "    return doc \n",
    "def remove_numbers(doc):\n",
    "    doc = re.sub(\"\\d+\", \"\", doc)\n",
    "    doc = doc.replace('m€', '')\n",
    "    doc = doc.replace('k€', '')   \n",
    "    return doc\n",
    "# Tokenize text\n",
    "def preprocessing(doc,train=False):\n",
    "        # Translator used to remove punctuation\n",
    "        translator = str.maketrans(' ', ' ', string.punctuation)\n",
    "\n",
    "        # Remove «»\n",
    "        doc = doc.replace(\"«\", \" \")\n",
    "        doc = doc.replace(\"»\", \" \")\n",
    "\n",
    "        # To lowercase \n",
    "        doc = doc.lower()\n",
    "        \n",
    "        # Remove url's\n",
    "        doc = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', doc, flags=re.MULTILINE)\n",
    "        \n",
    "        # Cleaning\n",
    "        doc = cleaning(doc)\n",
    "        \n",
    "        # Remove numbers\n",
    "        doc = remove_numbers(doc)\n",
    "        \n",
    "    \n",
    "        # Remove multiple wite spaces \n",
    "        doc = re.sub(' +', ' ',doc)\n",
    "        \n",
    "        # Remove unicode breaking character\n",
    "        doc = doc.replace(u'\\xa0', u' ')\n",
    "        \n",
    "        if train: \n",
    "            result = []\n",
    "            sentences = sent_tokenize(doc)\n",
    "            for sent in sentences: \n",
    "                   # Remove punctuation\n",
    "                sent = sent.translate(translator)\n",
    "                sent += \"\\n\"\n",
    "                result.append(sent)\n",
    "            return \"\".join(result)\n",
    "        else:\n",
    "            doc += \"\\n\"\n",
    "            return doc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "    return content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_file(temp_file,path):\n",
    "    subprocess.check_output([\"cat \" + temp_file + \"*\" + ' > ' + path],shell=True)\n",
    "    subprocess.check_output([\"rm \" + temp_file + \"*\"],shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_dataframe(df)\n",
    "df_prediction = filter_dataframe(df_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import time\n",
    "import subprocess\n",
    "def multi_thread_preprocessing(dataframe,path,train=True,threads=3):\n",
    "    temp_file_name = \"tmp_\"\n",
    "    chunks = np.array_split(dataframe,threads)\n",
    "    manager = multiprocessing.Manager()\n",
    "    threads = []\n",
    "    for index,chunk in enumerate(chunks):\n",
    "        thread = multiprocessing.Process(target=preprocess_and_write_to_file, args=(chunk,temp_file_name,train,index))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    if(train):\n",
    "        merge_file(temp_file_name,path)\n",
    "    else:\n",
    "        new_df = dataframe.copy()\n",
    "        temp_file_eval = 'eval_file_tmp'\n",
    "        merge_file(temp_file_name,temp_file_eval)\n",
    "        data = read_file(temp_file_eval)\n",
    "        print(len(data))\n",
    "        subprocess.run([\"rm\", temp_file_eval])\n",
    "        new_df['corpus'] = data\n",
    "        new_df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_and_write_to_file(dataframe,fileName='data',train=False,index=0):\n",
    "    total_len = (len(dataframe))\n",
    "    third = int(len(dataframe)/3)\n",
    "    f = codecs.open(fileName + str(index) + '.txt' , 'w', 'utf-8')\n",
    "    for counter,data in enumerate(dataframe.iterrows()):\n",
    "        i, row = data\n",
    "        if(counter%third==0):\n",
    "            print(\"Thread \" + str(index) + \" processed \" + str(counter) + \"/\" + str(total_len))\n",
    "        preprocessed_text = preprocessing((row[article]),train)\n",
    "        f.write(preprocessed_text)  # python will convert \\n to os.linesep\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_article</th>\n",
       "      <th>siren</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>http://www.boursier.com/actions/actualites/new...</td>\n",
       "      <td>421203993</td>\n",
       "      <td>Accès Industrie : renoue avec les bénéfices en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.agence-api.fr/article/novelis-pass...</td>\n",
       "      <td>421203993</td>\n",
       "      <td>Novelis passe dans le giron d’Accès industrie\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>http://www.boursier.com/actions/actualites/new...</td>\n",
       "      <td>421203993</td>\n",
       "      <td>Accès Industrie : en légère croissance sur 9 m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>http://www.boursier.com/actions/actualites/new...</td>\n",
       "      <td>421203993</td>\n",
       "      <td>Parquest dépose une offre ferme pour racheter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>http://www.boursier.com/actions/actualites/new...</td>\n",
       "      <td>421203993</td>\n",
       "      <td>Bonne surprise en début d'année pour Accès Ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>http://www.lefigaro.fr/medias/2018/01/19/20004...</td>\n",
       "      <td>432766947</td>\n",
       "      <td>Takis Candilis rejoint France Télévisions\\n  L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7271</th>\n",
       "      <td>http://www.cbnews.fr/digital/jo-d-hiver-2018-f...</td>\n",
       "      <td>432766947</td>\n",
       "      <td>JO d’hiver 2018 : France Télévisions lance un ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7272</th>\n",
       "      <td>http://www.cbnews.fr/mouvements/france-televis...</td>\n",
       "      <td>432766947</td>\n",
       "      <td>France Télévisions : Bertrand Scirpo délégué à...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7273</th>\n",
       "      <td>http://www.cbnews.fr/medias/france-televisions...</td>\n",
       "      <td>432766947</td>\n",
       "      <td>France Télévisions va réduire ses effectifs sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7274</th>\n",
       "      <td>http://www.lefigaro.fr/medias/france-televisio...</td>\n",
       "      <td>432766947</td>\n",
       "      <td>France Télévisions: Delphine Ernotte a fixé le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2535 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            url_article      siren  \\\n",
       "25    http://www.boursier.com/actions/actualites/new...  421203993   \n",
       "26    https://www.agence-api.fr/article/novelis-pass...  421203993   \n",
       "27    http://www.boursier.com/actions/actualites/new...  421203993   \n",
       "28    http://www.boursier.com/actions/actualites/new...  421203993   \n",
       "29    http://www.boursier.com/actions/actualites/new...  421203993   \n",
       "...                                                 ...        ...   \n",
       "7270  http://www.lefigaro.fr/medias/2018/01/19/20004...  432766947   \n",
       "7271  http://www.cbnews.fr/digital/jo-d-hiver-2018-f...  432766947   \n",
       "7272  http://www.cbnews.fr/mouvements/france-televis...  432766947   \n",
       "7273  http://www.cbnews.fr/medias/france-televisions...  432766947   \n",
       "7274  http://www.lefigaro.fr/medias/france-televisio...  432766947   \n",
       "\n",
       "                                                 corpus  \n",
       "25    Accès Industrie : renoue avec les bénéfices en...  \n",
       "26    Novelis passe dans le giron d’Accès industrie\\...  \n",
       "27    Accès Industrie : en légère croissance sur 9 m...  \n",
       "28    Parquest dépose une offre ferme pour racheter ...  \n",
       "29    Bonne surprise en début d'année pour Accès Ind...  \n",
       "...                                                 ...  \n",
       "7270  Takis Candilis rejoint France Télévisions\\n  L...  \n",
       "7271  JO d’hiver 2018 : France Télévisions lance un ...  \n",
       "7272  France Télévisions : Bertrand Scirpo délégué à...  \n",
       "7273  France Télévisions va réduire ses effectifs sa...  \n",
       "7274  France Télévisions: Delphine Ernotte a fixé le...  \n",
       "\n",
       "[2535 rows x 3 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0 processed 0/845\n",
      "Thread 1 processed 0/845\n",
      "Thread 2 processed 0/845\n",
      "Thread 0 processed 281/845\n",
      "Thread 2 processed 281/845\n",
      "Thread 1 processed 281/845\n",
      "Thread 0 processed 562/845\n",
      "Thread 2 processed 562/845\n",
      "Thread 1 processed 562/845\n",
      "Thread 2 processed 843/845\n",
      "Thread 0 processed 843/845\n",
      "Thread 1 processed 843/845\n",
      "2535\n"
     ]
    }
   ],
   "source": [
    "multi_thread_preprocessing(df_prediction,prediction_file,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0 processed 0/6432\n",
      "Thread 1 processed 0/6432\n",
      "Thread 2 processed 0/6431\n",
      "Thread 0 processed 2144/6432\n",
      "Thread 1 processed 2144/6432\n",
      "Thread 2 processed 2143/6431\n",
      "Thread 0 processed 4288/6432\n",
      "Thread 2 processed 4286/6431\n",
      "Thread 1 processed 4288/6432\n",
      "Thread 2 processed 6429/6431\n",
      "19295\n"
     ]
    }
   ],
   "source": [
    "multi_thread_preprocessing(df,eval_file,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0 processed 0/6432\n",
      "Thread 1 processed 0/6432\n",
      "Thread 2 processed 0/6431\n",
      "Thread 0 processed 2144/6432\n",
      "Thread 2 processed 2143/6431\n",
      "Thread 1 processed 2144/6432\n",
      "Thread 0 processed 4288/6432\n",
      "Thread 2 processed 4286/6431\n",
      "Thread 1 processed 4288/6432\n"
     ]
    }
   ],
   "source": [
    "multi_thread_preprocessing(df,train_path,train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
