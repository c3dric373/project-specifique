{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%m-%Y\")\n",
    "#dt_string = \"13-05-2020\"\n",
    "global_path = \"../models/\"\n",
    "model_path = \".model\"\n",
    "txt_extension = \".txt\"\n",
    "path_ft =  global_path + \"ft_\"+ dt_string + txt_extension\n",
    "path_w2v =  global_path + \"w2v_\"+ dt_string + model_path\n",
    "path_glove = global_path + \"gloVe_\"+ dt_string + txt_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile,datapath\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "model_ft = KeyedVectors.load_word2vec_format(path_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile,datapath\n",
    "from gensim.models import Word2Vec\n",
    "path = get_tmpfile(path_w2v)\n",
    "model_w2v = Word2Vec.load(path_w2v)\n",
    "model_w2v = model_w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile,datapath\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "model_glove = KeyedVectors.load_word2vec_format(path_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('../data/eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = (df[(df['legal_name'] == \"THERADIAG SA\")])\n",
    "#df=df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>siren</th>\n",
       "      <th>legal_name</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>419838529</td>\n",
       "      <td>IPSEN</td>\n",
       "      <td>ipsen lorgne les peptides de peptimimesis \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>419838529</td>\n",
       "      <td>IPSEN</td>\n",
       "      <td>ipsen accord important avec probi publi√© le √† ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>419838529</td>\n",
       "      <td>IPSEN</td>\n",
       "      <td>la m√©decine g√©n√©rale dipsen en panne au er tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>419838529</td>\n",
       "      <td>IPSEN</td>\n",
       "      <td>bourse en ligne information boursiere economie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>419838529</td>\n",
       "      <td>IPSEN</td>\n",
       "      <td>bourse en ligne information boursiere economie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25523</th>\n",
       "      <td>25523</td>\n",
       "      <td>58371</td>\n",
       "      <td>489682005</td>\n",
       "      <td>ZENIKA</td>\n",
       "      <td>website unavailable this site is currently sus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25524</th>\n",
       "      <td>25524</td>\n",
       "      <td>58372</td>\n",
       "      <td>489682005</td>\n",
       "      <td>ZENIKA</td>\n",
       "      <td>website unavailable this site is currently sus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25525</th>\n",
       "      <td>25525</td>\n",
       "      <td>58373</td>\n",
       "      <td>489682005</td>\n",
       "      <td>ZENIKA</td>\n",
       "      <td>isatis capital entre au capital de zenika fusa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25526</th>\n",
       "      <td>25526</td>\n",
       "      <td>58374</td>\n",
       "      <td>489682005</td>\n",
       "      <td>ZENIKA</td>\n",
       "      <td>cdi ux designer hf lyon zenika uxjobsfr tu te ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25527</th>\n",
       "      <td>25527</td>\n",
       "      <td>58375</td>\n",
       "      <td>489682005</td>\n",
       "      <td>ZENIKA</td>\n",
       "      <td>zenika bordeaux sur twitter jobs helloüôãüèª nous ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25528 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1      siren legal_name  \\\n",
       "0               0             0  419838529      IPSEN   \n",
       "1               1             1  419838529      IPSEN   \n",
       "2               2             2  419838529      IPSEN   \n",
       "3               3             3  419838529      IPSEN   \n",
       "4               4             4  419838529      IPSEN   \n",
       "...           ...           ...        ...        ...   \n",
       "25523       25523         58371  489682005     ZENIKA   \n",
       "25524       25524         58372  489682005     ZENIKA   \n",
       "25525       25525         58373  489682005     ZENIKA   \n",
       "25526       25526         58374  489682005     ZENIKA   \n",
       "25527       25527         58375  489682005     ZENIKA   \n",
       "\n",
       "                                                  corpus  \n",
       "0           ipsen lorgne les peptides de peptimimesis \\n  \n",
       "1      ipsen accord important avec probi publi√© le √† ...  \n",
       "2      la m√©decine g√©n√©rale dipsen en panne au er tri...  \n",
       "3      bourse en ligne information boursiere economie...  \n",
       "4      bourse en ligne information boursiere economie...  \n",
       "...                                                  ...  \n",
       "25523  website unavailable this site is currently sus...  \n",
       "25524  website unavailable this site is currently sus...  \n",
       "25525  isatis capital entre au capital de zenika fusa...  \n",
       "25526  cdi ux designer hf lyon zenika uxjobsfr tu te ...  \n",
       "25527  zenika bordeaux sur twitter jobs helloüôãüèª nous ...  \n",
       "\n",
       "[25528 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to store results\n",
    "\n",
    "df['eval_set_w2v'] = \"\"\n",
    "df['eval_set_ft'] = \"\"\n",
    "df['eval_set_glove'] = \"\"\n",
    "df['eval_number_w2v'] = \"\"\n",
    "df['eval_number_ft'] = \"\"\n",
    "df['eval_number_glove'] = \"\"\n",
    "df['corpus'] = df['corpus'].apply(lambda corpus: set(corpus.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get name of all companies in eval dataset \n",
    "vocab = set(df.legal_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map SIREN to legal name and vice versa\n",
    "from collections import defaultdict\n",
    "siren_to_legal_name = defaultdict(str)\n",
    "legal_n_to_siren = defaultdict(str)\n",
    "for i,row in df.iterrows():\n",
    "    siren_to_legal_name[row.siren] = row.legal_name\n",
    "    legal_n_to_siren[row.legal_name] = row.siren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Result Dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load common names dictionnary\n",
    "file = open('../pickledObjects/legal_to_common_names.obj', 'rb') \n",
    "legal_to_common_name = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_eval_set(dataframe, legal_name, nearest_words,method):\n",
    "    occurences_word = defaultdict(int)\n",
    "    df_legal_name = dataframe[(dataframe['legal_name'] == legal_name)]\n",
    "    for i,row in df_legal_name.iterrows():\n",
    "        nearest_words_set = set(nearest_words)\n",
    "        corpus_set = row.corpus\n",
    "        inters_ = nearest_words_set.intersection(corpus_set)\n",
    "        if(method == 'w2v'):\n",
    "            df['eval_set_w2v'][i] =  list(inters_)\n",
    "            df['eval_number_w2v'][i] = len(inters_)\n",
    "        elif(method == 'ft'):\n",
    "            df['eval_set_ft'][i] =  list(inters_)\n",
    "            df['eval_number_ft'][i] = len(inters_)\n",
    "        else:\n",
    "            df['eval_set_glove'][i] =  list(inters_)\n",
    "            df['eval_number_glove'][i] = len(inters_)\n",
    "        for word in inters_:\n",
    "                occurences_word[word] +=1\n",
    "    return occurences_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_dict(word):\n",
    "    emb_dict = defaultdict(list)\n",
    "    emb_dict['ft'] = get_most_similar(word,model_ft)\n",
    "    emb_dict['w2v'] = get_most_similar(word,model_w2v)\n",
    "    emb_dict['glove'] = get_most_similar(word,model_glove)\n",
    "    return emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_ft(word):\n",
    "    nearest_words  = model_ft.get_nearest_neighbors(word.lower(),10)\n",
    "    return [[(y),round(x,2)] for x,y in nearest_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_composed_word_vector(composed_word,wordVectors): \n",
    "    res = []\n",
    "    for word in composed_word.split():\n",
    "        if(word.lower() in wordVectors.vocab):\n",
    "            res.append(wordVectors[word.lower()])\n",
    "    if(len(res)==0):\n",
    "        return res \n",
    "    return np.mean(res,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(word,model,topn=10):\n",
    "    res = []\n",
    "    if(len(word.split(\" \")) > 0):\n",
    "        vector = get_composed_word_vector(word,model_glove)\n",
    "    else: \n",
    "        vector = model[word.lower()]\n",
    "    if(len(vector)>0):\n",
    "        nearest_words = model.most_similar([vector], topn=topn)\n",
    "        res =  [[(x),round(y,2)] for x,y in nearest_words]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score2(legal_name,word_occurences):\n",
    "    return round((word_occurences / len(df[(df['legal_name'] == legal_name)])),4)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_indices(value, qlist):\n",
    "    indices = []\n",
    "    idx = -1\n",
    "    while True:\n",
    "        try:\n",
    "            idx = qlist.index(value, idx+1)\n",
    "            indices.append(idx)\n",
    "        except ValueError:\n",
    "            break\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score2(dataframe,nearest_words, legal_name,method):\n",
    "    occurences_words = add_eval_set(df, legal_name,[x[0] for x in nearest_words],method)\n",
    "    for word in nearest_words: \n",
    "        word.append(score2(legal_name, occurences_words[word[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_from_dict(dict_,legal_name,method):\n",
    "    res = []\n",
    "    common_names_dict = dict_[legal_name] \n",
    "    for common_name in common_names_dict.keys():\n",
    "        #print(dict_[legal_name][common_name])\n",
    "        res.append(dict_[legal_name][common_name][method])\n",
    "    return [y for x in res for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTime(start,end):\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "    return time_since_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion(nearest_words):\n",
    "    word_list = [x[0] for x in nearest_words]\n",
    "    res = []\n",
    "    list_of_pairs = []\n",
    "    for i,word in enumerate(word_list):\n",
    "        indices_word = all_indices(word,word_list)\n",
    "        list_of_pairs.append(indices_word)\n",
    "    list_of_pairs = [list(item) for item in set(tuple(row) for row in list_of_pairs)]\n",
    "    for pairs in list_of_pairs:\n",
    "        if(len(pairs)>1):\n",
    "            res.append(fusion_list([x for j,x in enumerate(nearest_words) if j in pairs ]))\n",
    "        else: \n",
    "            res.append(nearest_words[pairs[0]])\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_list(l):\n",
    "    new_similarity = round(sum([x[1] for x in l]),3)\n",
    "    return [l[0][0], new_similarity, l[0][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%0%, Time:  00:00:00.00\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-520e0068ad97>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['eval_set_ft'][i] =  list(inters_)\n",
      "<ipython-input-13-520e0068ad97>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['eval_number_ft'][i] = len(inters_)\n",
      "<ipython-input-13-520e0068ad97>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['eval_set_w2v'][i] =  list(inters_)\n",
      "<ipython-input-13-520e0068ad97>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['eval_number_w2v'][i] = len(inters_)\n",
      "<ipython-input-13-520e0068ad97>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['eval_set_glove'][i] =  list(inters_)\n",
      "<ipython-input-13-520e0068ad97>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['eval_number_glove'][i] = len(inters_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%==========100%, Time:  00:03:54.95\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "nearest = defaultdict(dict)\n",
    "new_nearest = defaultdict(dict)\n",
    "methods = ['ft','w2v','glove']\n",
    "# Create dictionnary with nearest vectors\n",
    "percent = 0\n",
    "start = time.time()\n",
    "for i,legal_name in enumerate(vocab): \n",
    "    # Logging\n",
    "    if(i % 150 == 0):\n",
    "        time_ = getTime(start,time.time())\n",
    "        print(\"0%\" + \"=\" *(int(percent/10))+ str(percent) +\"%, \" + time_, end=\"\\r\" )\n",
    "        percent +=10\n",
    "    legal_name_dict = defaultdict(dict)\n",
    "    nearest[legal_name] = legal_name_dict\n",
    "    siren = legal_n_to_siren[legal_name]\n",
    "    if(siren in legal_to_common_name.keys()):\n",
    "        common_names = legal_to_common_name[siren]\n",
    "        # For each common name compute nearest words per embedding technique \n",
    "        for common_n in common_names:\n",
    "            legal_name_dict[common_n] = create_emb_dict(common_n)     \n",
    "    else:\n",
    "        legal_name_dict[legal_name] = emb_dict\n",
    "    legal_name_dict = defaultdict(list)\n",
    "    new_nearest[legal_name] = legal_name_dict\n",
    "    for method in methods:\n",
    "        fusioned_dict = defaultdict(list)\n",
    "        # Compute score2\n",
    "        compute_score2(df,get_nearest_from_dict(nearest,legal_name,method),legal_name,method)\n",
    "        # Fusion result of each common name\n",
    "        nearest_fusioned = fusion(get_nearest_from_dict(nearest,legal_name,method))\n",
    "        #nearest_fusioned\n",
    "        legal_name_dict[method] = nearest_fusioned\n",
    "        # Compute Gobal Score to evaluate method\n",
    "        sgx1 = round(sum(x[1] for x in nearest_fusioned),3)\n",
    "        sgx2 = round(sum(x[2]for x in nearest_fusioned),3)\n",
    "        sgx3 = sgx1*sgx2\n",
    "        scores = [('sgx1',sgx1),('sgx2',sgx2),('sgx3',sgx3)]\n",
    "        legal_name_dict[method].insert(0,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results' + dt_string + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056878721403948604\n",
      "0.2824349733625823\n",
      "1.8756659354434346\n"
     ]
    }
   ],
   "source": [
    "ftEval = df['eval_number_ft'].mean()\n",
    "w2vEval = df['eval_number_w2v'].mean()\n",
    "gloveEval = df['eval_number_glove'].mean()\n",
    "print(ftEval)\n",
    "print(w2vEval)\n",
    "print(gloveEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "\n",
    "with codecs.open('results-with-fusion.json', 'w',indent=4, sort_keys=True,encoding='utf-8') as fp:\n",
    "    json.dump(nearest,fp,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
